{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fS9qdPeO5Yt"
   },
   "source": [
    "# 2D Classification Example on MedNIST and rising\n",
    "Welcome to this rising example, where we will build a 2D classification pipeline with rising and pyorch lightning. The dataset part of this notebook was inspired by the [Monai MedNIST](https://colab.research.google.com/drive/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe#scrollTo=ZaHFhidyCBJa) example, so make sure to check them out, too :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jupyMjlQPN3M"
   },
   "source": [
    "## Preparation\n",
    "Let's start with some basic preparations of our environment and download the MedNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSyfsP8WCmpa"
   },
   "source": [
    "First, we will install rising's master branch to get the latest features (if your a not planning to extend rising you can easily install out pypi package with `pip install rising`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1muo5F1dRKTz"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet git+https://github.com/PhoenixDL/rising # for data handling\n",
    "!pip install --upgrade --quiet pytorch-lightning # for easy training\n",
    "!pip install --upgrade --quiet scikit-learn # for classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoi8GG4qCGeT"
   },
   "source": [
    "Next, we will add some magic to our notebook in case your are running them locally and do not want refresh it all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pc7NuLJaS4KT"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XZymts0CZKk"
   },
   "source": [
    "Finally, we download the MedNIST dataset and undpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xws3CJmHOzrQ"
   },
   "outputs": [],
   "source": [
    "!curl -L -o MedNIST.tar.gz 'https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz'\n",
    "\n",
    "# unzip the '.tar.gz' file to the current directory\n",
    "import tarfile\n",
    "datafile = tarfile.open(\"MedNIST.tar.gz\")\n",
    "datafile.extractall()\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "benfLVjWCgek"
   },
   "source": [
    "## Preparing our datasets\n",
    "If you already wrote your own datasets with PyTorch this well be very familiar because `rising` uses the same dataset structure as PyTorch. The only difference between native PyTorch and `rising` is the transformation part. While PyTorch embeds its transformation into the dataset, we opted to move the transformations to our dataloder (which is a direct subclass of PyTorch's dataloader) to make our datasets easily interchangeable between multiple tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3N6Upt6_Dntn"
   },
   "source": [
    "Let's start by searching for the paths of the image files and defining their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBU0MklpPLhc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = Path('./MedNIST/')\n",
    "class_names = sorted([p.stem for p in data_dir.iterdir() if p.is_dir()])\n",
    "num_class = len(class_names)\n",
    "\n",
    "image_files = [[x for x in (data_dir / class_name).iterdir()] for class_name in class_names]\n",
    "\n",
    "image_file_list = []\n",
    "image_label_list = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    image_file_list.extend(image_files[i])\n",
    "    image_label_list.extend([i] * len(image_files[i]))\n",
    "\n",
    "num_total = len(image_label_list)\n",
    "\n",
    "print('Total image count:', num_total)\n",
    "print(\"Label names:\", class_names)\n",
    "print(\"Label counts:\", [len(image_files[i]) for i in range(num_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset size during our CI\n",
    "if 'CI' in os.environ:\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    red_samples = int(0.01 * len(image_file_list))\n",
    "    idx = random.choices(range(len(image_file_list)), k=red_samples)\n",
    "    image_file_list = [image_file_list[i] for i in idx]\n",
    "    image_label_list = [image_label_list[i] for i in idx]\n",
    "    num_total = len(image_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QG1DFaOHK2KO"
   },
   "source": [
    "The output should look like this:\n",
    "```\n",
    "Total image count: 58954\n",
    "Label names: ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']\n",
    "Label counts: [10000, 8954, 10000, 10000, 10000, 10000]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FS5jhz40Ffex"
   },
   "source": [
    "The downloaded data needs to be divided into 3 subsets for training, validation and testing. Because the dataset is fairly large we can opt for an 80/10/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sJ3b4m_PRWy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_frac, test_frac = 0.1, 0.1\n",
    "trainX, trainY = [], []\n",
    "valX, valY = [], []\n",
    "testX, testY = [], []\n",
    "\n",
    "for i in range(num_total):\n",
    "    rann = np.random.random()\n",
    "    if rann < valid_frac:\n",
    "        valX.append(image_file_list[i])\n",
    "        valY.append(image_label_list[i])\n",
    "    elif rann < test_frac + valid_frac:\n",
    "        testX.append(image_file_list[i])\n",
    "        testY.append(image_label_list[i])\n",
    "    else:\n",
    "        trainX.append(image_file_list[i])\n",
    "        trainY.append(image_label_list[i])\n",
    "\n",
    "print(\"Training count =\",len(trainX),\"Validation count =\", len(valX), \"Test count =\",len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8ThnxY_F1-c"
   },
   "source": [
    "The MedNIST dataset now just needs to load the specified files. We use PIL to load the individual image file and convert it to a tensor afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J054LfzYPgLw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import Sequence, Dict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MedNISTDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Simple dataset to load individual samples from the dataset\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, image_files: Sequence[str], labels: Sequence[int]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      image_files: paths to the image files\n",
    "      labels: label for each file\n",
    "    \"\"\"\n",
    "    assert len(image_files) == len(labels), \"Every file needs a label\"\n",
    "    self.image_files = image_files\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    \"\"\"\n",
    "    Number of samples inside the dataset\n",
    "\n",
    "    Returns:\n",
    "      int: length\n",
    "    \"\"\"\n",
    "    return len(self.image_files)\n",
    "\n",
    "  def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Select an individual sample from the dataset\n",
    "\n",
    "    Args:\n",
    "      index: index of sample to draw\n",
    "\n",
    "    Return:\n",
    "      Dict[str, torch.Tensor]: single sample\n",
    "        * `data`: image data\n",
    "        * `label`: label for sample\n",
    "    \"\"\"\n",
    "    data_np = np.array(Image.open(self.image_files[index]))\n",
    "    return {\"data\": torch.from_numpy(data_np)[None].float(),\n",
    "            \"label\": torch.tensor(self.labels[index]).long()}\n",
    "\n",
    "train_ds = MedNISTDataset(trainX, trainY)\n",
    "val_ds = MedNISTDataset(valX, valY)\n",
    "test_ds = MedNISTDataset(testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DO5p_kFHFil"
   },
   "source": [
    "Let see some basic statistics of a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3pR83ziW2FJ"
   },
   "outputs": [],
   "source": [
    "print(f'Single image min: {train_ds[0][\"data\"].min()}')\n",
    "print(f'Single image max: {train_ds[0][\"data\"].max()}')\n",
    "print(f'Single image mean: {train_ds[0][\"data\"].shape} (C, W, H)')\n",
    "print(f'Exaple label {train_ds[0][\"label\"]}')\n",
    "print(f'Example data: {train_ds[0][\"data\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puJJWtrrKtE3"
   },
   "source": [
    "The output could look something like this:\n",
    "```\n",
    "Single image min: 87.0\n",
    "Single image max: 255.0\n",
    "Single image mean: torch.Size([1, 64, 64]) (C, W, H)\n",
    "Exaple label 0\n",
    "Example data: tensor([[[101., 101., 101.,  ..., 101., 101., 101.],\n",
    "         [101., 101., 101.,  ..., 101., 101., 101.],\n",
    "         [101., 101., 101.,  ..., 101., 101., 101.],\n",
    "         ...,\n",
    "         [102., 101.,  99.,  ..., 111., 103.,  98.],\n",
    "         [102., 101., 100.,  ...,  99.,  98.,  98.],\n",
    "         [ 99., 100., 102.,  ..., 101., 103., 105.]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8pFCTCdIIfa"
   },
   "source": [
    "## Setting Up our Dataloading and Transformations\n",
    "In this section we will define our transformations and plug our dataset into the dataloader of `rising`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RheDTBMJdtE"
   },
   "source": [
    "First we setup our transformation. In general these can be split into two parts: transformations which are applied as preprocessing and transformations which are applied as augmentations. All transformations are applied in a batched fashion to the dataset to fully utilize vectorization to speed up augmentation. In case your dataset needs additional preprocessing on a per sample basis you can also add those to the dataloder with `sample_transforms`. Check out or [3D Segmentation Tutorial](https://github.com/PhoenixDL/rising/blob/master/notebooks/lightning_segmentation.ipynb) for more infroamtion about that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQpBi2vwP0lt"
   },
   "outputs": [],
   "source": [
    "import rising.transforms as rtr\n",
    "from rising.random import UniformParameter\n",
    "\n",
    "transforms_prep = []\n",
    "transforms_augment = []\n",
    "\n",
    "# preprocessing transforms\n",
    "# transforms_prep.append(rtr.NormZeroMeanUnitStd())\n",
    "transforms_prep.append(rtr.NormMinMax()) # visualization looks nicer :) \n",
    "\n",
    "# augmentation transforms\n",
    "transforms_augment.append(rtr.GaussianNoise(0., 0.01))\n",
    "transforms_augment.append(rtr.GaussianSmoothing(\n",
    "    in_channels=1, kernel_size=3, std=0.5, padding=1))\n",
    "transforms_augment.append(rtr.Rot90((0, 1)))\n",
    "transforms_augment.append(rtr.Mirror(dims=(0, 1)))\n",
    "transforms_augment.append(rtr.BaseAffine(\n",
    "    scale=UniformParameter(0.8, 1.2),\n",
    "    rotation=UniformParameter(-30, 30), degree=True,\n",
    "    # translation in base affine is normalized to image size\n",
    "    # Translation transform offers to option to swith to pixels\n",
    "    translation=UniformParameter(-0.02, 0.02), \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mf_2-bXHLcYR"
   },
   "source": [
    "In contrast to native PyTorch we add our transformations to the dataloder of rising. There are three main types of transformations which can be added:\n",
    "* `sample_transforms`: these transforms are applied per sample. In case the transformation assumes a batch of data `pseudo_batch_dim` can be activated to\n",
    "automatically add a batch dim to single samples.\n",
    "* `batch_transforms`: these transforms are executed per batch inside the multiprocessig context of the CPU (like `sample_transforms`).\n",
    "* `gpu_transforms`: these transforms are executed on the GPU. In case you have\n",
    "multiple GPUs make sure to set the correct `device`, otherwise rising could use\n",
    "the wrong GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQOFnEbWQB8r"
   },
   "outputs": [],
   "source": [
    "from rising.loading import DataLoader\n",
    "\n",
    "tr_transform = rtr.Compose(transforms_prep + transforms_augment)\n",
    "dataloader_tr = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
    "                           gpu_transforms=tr_transform)\n",
    "\n",
    "val_transform = rtr.Compose(transforms_prep)\n",
    "dataloader_val = DataLoader(val_ds, batch_size=32,\n",
    "                            gpu_transforms=val_transform)\n",
    "\n",
    "test_transform = rtr.Compose(transforms_prep)\n",
    "dataloader_ts = DataLoader(test_ds, batch_size=32,\n",
    "                           gpu_transforms=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYQMemJMLWm0"
   },
   "source": [
    "### Looking at some example outputs\n",
    "In this short section we will visualize some of the batches to look at the influence of the augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-BvnKQVSu8Z"
   },
   "outputs": [],
   "source": [
    "# helper function to visualize batches of images\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(batch: torch.Tensor, norm: bool = True):\n",
    "  \"\"\"\n",
    "  Visualize a single batch of images\n",
    "\n",
    "  Args:\n",
    "    batch: batch of data\n",
    "    norm: normalized to range 0,1 for visualization purposes\n",
    "  \"\"\"\n",
    "  grid = torchvision.utils.make_grid(batch.cpu(), nrow=8)\n",
    "\n",
    "  grid -= grid.min()\n",
    "  m = grid.max()\n",
    "  if m > 1e-6:\n",
    "    grid = grid / m\n",
    "\n",
    "  plt.figure(figsize=(10,5))\n",
    "  plt.imshow(grid[0], cmap='gray', vmin=0, vmax=1)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEluCh7uQkKG"
   },
   "outputs": [],
   "source": [
    "# make dataset iterable\n",
    "_iter = iter(dataloader_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plgecEHcUM1a"
   },
   "outputs": [],
   "source": [
    "# visualize batch of images\n",
    "batch = next(_iter)\n",
    "print({f'{key}_shape: {tuple(batch[key].shape)}' for key, item in batch.items()})\n",
    "print(f'Batch labels: \\n{batch[\"label\"]}')\n",
    "print(f'Batch mean {batch[\"data\"].mean()}')\n",
    "print(f'Batch min {batch[\"data\"].min()}')\n",
    "print(f'Batch max {batch[\"data\"].max()}')\n",
    "\n",
    "show_batch(batch[\"data\"], norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NMZnxKiRMpY"
   },
   "source": [
    "The output of the visualization could look something like this:\n",
    "\n",
    "![Example Batch](https://drive.google.com/uc?id=1OEAcmtGJ5nQJLcu_BriL5zMIK_WxzHmZ)\n",
    "\n",
    "The exact images will vary because the batch was selected from the training dataloader which shuffles the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlFB8j3gLP-k"
   },
   "source": [
    "## Defining our Lightning Module\n",
    "We will use [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) as our trainer framework to save some time and to standardize our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTmIVC_KS9QX"
   },
   "source": [
    "In lightning the training models are derived from `pytorch_lightning.LightningModule` which enforces a specific structure of the code to increase reproducibility and stardization across the community. For simplicity we will simply load a torchvision model and overwrite the basic `*_step` functions of lightning. If you want more information how to build pipelines with pytorch lightning, please check out their [documentation](https://github.com/PyTorchLightning/pytorch-lightning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "if 'CI' in os.environ:\n",
    "    # use a very small model for CI\n",
    "    class SuperSmallModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
    "            self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "            self.pool1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(32, num_class)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = torch.flatten(self.pool1(x), 1)\n",
    "            return self.fc(x)\n",
    "    resnet = SuperSmallModel()\n",
    "else:\n",
    "    # resnet18 for normal runs\n",
    "    resnet = models.resnet18(pretrained=False)\n",
    "    # change first layer\n",
    "    resnet.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # change last layer\n",
    "    fc_in = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(fc_in, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnLgukDsUUy5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class SimpleClassifier(pl.LightningModule):\n",
    "  def __init__(self, hparams: Optional[dict] = None):\n",
    "    \"\"\"\n",
    "    Hyperparameters for our model\n",
    "\n",
    "    Args:\n",
    "      hparams: hyperparameters for model\n",
    "        `lr`: learning rate for optimizer\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    if hparams is None:\n",
    "        hparams = {}\n",
    "    self.hparams = hparams\n",
    "    self.model = resnet\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward input batch of data through model\n",
    "\n",
    "    Args:\n",
    "      x: input batch of data [N, C, H, W]\n",
    "        N batch size (here 32); C number of channels (here 1);\n",
    "        H,W spatial dimensions of images (here 64x64)\n",
    "    \n",
    "    Returns:\n",
    "      torch.Tensor: classification logits [N, num_classes]\n",
    "    \"\"\"\n",
    "    return self.model(x)\n",
    "\n",
    "  def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Forward batch and compute loss for a single step (used for training)\n",
    "\n",
    "    Args:\n",
    "      batch: batch to process\n",
    "        `data`: input data\n",
    "        `label`: expected labels\n",
    "      batch_idx: index of batch\n",
    "    \"\"\"\n",
    "    x, y = batch[\"data\"], batch[\"label\"]\n",
    "    y_hat = self(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    tensorboard_logs = {'train_loss': loss}\n",
    "    return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Forward batch and compute loss for a single step (used for validation)\n",
    "\n",
    "    Args:\n",
    "      batch: batch to process\n",
    "        `data`: input data\n",
    "        `label`: expected labels\n",
    "      batch_idx: index of batch\n",
    "    \"\"\"\n",
    "    x, y = batch[\"data\"], batch[\"label\"]\n",
    "    y_hat = self(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y)\n",
    "    return {'val_loss': val_loss}\n",
    "\n",
    "  def validation_epoch_end(self, outputs):\n",
    "    \"\"\"\n",
    "    Compute average validation loss during epoch\n",
    "    \"\"\"\n",
    "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    tensorboard_logs = {'val_loss': avg_loss}\n",
    "    return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "      \n",
    "  def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Forward batch and compute loss for a single step (used for validation)\n",
    "\n",
    "    Args:\n",
    "      batch: batch to process\n",
    "        `data`: input data\n",
    "        `label`: expected labels\n",
    "      batch_idx: index of batch\n",
    "    \"\"\"\n",
    "    x, y = batch[\"data\"], batch[\"label\"]\n",
    "    y_hat = self(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y)\n",
    "    return {'test_loss': val_loss,\n",
    "            \"pred_label\": y_hat.max(dim=1)[1].detach().cpu(),\n",
    "            \"label\": y.detach().cpu()}\n",
    "\n",
    "  def test_epoch_end(self, outputs):\n",
    "    \"\"\"\n",
    "    Compute average test loss and classification metrics\n",
    "    \"\"\"\n",
    "    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "    tensorboard_logs = {'test_loss': avg_loss}\n",
    "\n",
    "    all_pred_label = torch.cat([x['pred_label'] for x in outputs])\n",
    "    all_label = torch.cat([x['label'] for x in outputs])\n",
    "    print(classification_report(all_label.numpy(),\n",
    "                                all_pred_label.numpy(),\n",
    "                                target_names=class_names, digits=4))\n",
    "\n",
    "    return {'test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"\n",
    "    Setup optimizer for training\n",
    "    \"\"\"\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.hparams.get(\"lr\", 1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbV8bfFjS8bE"
   },
   "source": [
    "We can visualize our training progress and hyperparameters in tensorboard to easily compare multiple runs of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYQi3zanWgde"
   },
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VWpsbBES38n"
   },
   "source": [
    "Let's start our training :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZh2Zf-0Vor2"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = SimpleClassifier()\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(gpus=None, progress_bar_refresh_rate=10, max_epochs=4, weights_summary=None)\n",
    "trainer.fit(model, train_dataloader=dataloader_tr, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMmIOVEJW-Wo"
   },
   "source": [
    "After training our model we can test it on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdX_PB6tXAjA"
   },
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders=dataloader_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIYGzjLyTNNM"
   },
   "source": [
    "The results on the test data should look similar to this:\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "   AbdomenCT     0.9536    0.9990    0.9758      1008\n",
    "   BreastMRI     1.0000    1.0000    1.0000       830\n",
    "         CXR     0.9960    0.9872    0.9916      1013\n",
    "     ChestCT     1.0000    0.9490    0.9738       961\n",
    "        Hand     0.9877    0.9887    0.9882       975\n",
    "      HeadCT     0.9912    1.0000    0.9956      1019\n",
    "\n",
    "    accuracy                         0.9873      5806\n",
    "   macro avg     0.9881    0.9873    0.9875      5806\n",
    "weighted avg     0.9876    0.9873    0.9872      5806\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MedNIST_rising_lightning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
