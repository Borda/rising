{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MedNIST_rising_lightning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fS9qdPeO5Yt",
        "colab_type": "text"
      },
      "source": [
        "# 2D Classification Example on MedNIST and rising\n",
        "Welcome to this rising example, where we will build a 2D classification pipeline with rising and pyorch lightning. The dataset part of this notebook was inspired by the [Monai MedNIST](https://colab.research.google.com/drive/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe#scrollTo=ZaHFhidyCBJa) example, so make sure to check them out, too :D "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jupyMjlQPN3M",
        "colab_type": "text"
      },
      "source": [
        "## Preparation\n",
        "Let's start with some basic preparations of our environment and download the MedNIST data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSyfsP8WCmpa",
        "colab_type": "text"
      },
      "source": [
        "First, we will install rising's master branch to get the latest features (if your a not planning to extend rising you can easily install out pypi package with `pip install rising`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1muo5F1dRKTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade --quiet git+https://github.com/PhoenixDL/rising # for data handling\n",
        "!pip install --upgrade --quiet pytorch-lightning # for easy training\n",
        "!pip install --upgrade --quiet scikit-learn # for classification metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoi8GG4qCGeT",
        "colab_type": "text"
      },
      "source": [
        "Next, we will add some magic to our notebook in case your are running them locally and do not want refreash it all the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc7NuLJaS4KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XZymts0CZKk",
        "colab_type": "text"
      },
      "source": [
        "Finally, we download the MedNIST dataset and undpack it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xws3CJmHOzrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz\n",
        "\n",
        "# unzip the '.tar.gz' file to the current directory\n",
        "import tarfile\n",
        "datafile = tarfile.open(\"MedNIST.tar.gz\")\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benfLVjWCgek",
        "colab_type": "text"
      },
      "source": [
        "## Preparing our datasets\n",
        "If you already wrote your own datasets with PyTorch this well be very familiar because `rising` uses the same dataset structure as PyTorch. The only difference between native PyTorch and `rising` is the transformation part. While PyTorch embeds its transformation into the dataset, we opted to move the transformations to our dataloder (which is a direct subclass of PyTorch's dataloader) to make our datasets easily interchangeable between multiple tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N6Upt6_Dntn",
        "colab_type": "text"
      },
      "source": [
        "Let's start by searching for the paths of the image files and defining their classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBU0MklpPLhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "data_dir = Path('./MedNIST/')\n",
        "class_names = sorted([p.stem for p in data_dir.iterdir() if p.is_dir()])\n",
        "num_class = len(class_names)\n",
        "\n",
        "image_files = [[x for x in (data_dir / class_name).iterdir()] for class_name in class_names]\n",
        "\n",
        "image_file_list = []\n",
        "image_label_list = []\n",
        "for i, class_name in enumerate(class_names):\n",
        "    image_file_list.extend(image_files[i])\n",
        "    image_label_list.extend([i] * len(image_files[i]))\n",
        "\n",
        "num_total = len(image_label_list)\n",
        "\n",
        "print('Total image count:', num_total)\n",
        "print(\"Label names:\", class_names)\n",
        "print(\"Label counts:\", [len(image_files[i]) for i in range(num_class)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG1DFaOHK2KO",
        "colab_type": "text"
      },
      "source": [
        "The output should look like this:\n",
        "```\n",
        "Total image count: 58954\n",
        "Label names: ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']\n",
        "Label counts: [10000, 8954, 10000, 10000, 10000, 10000]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS5jhz40Ffex",
        "colab_type": "text"
      },
      "source": [
        "The downloaded data needs to be divided into 3 subsets for training, validation and testing. Because the dataset is fairly large we can opt for an 80/10/10 split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sJ3b4m_PRWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "valid_frac, test_frac = 0.1, 0.1\n",
        "trainX, trainY = [], []\n",
        "valX, valY = [], []\n",
        "testX, testY = [], []\n",
        "\n",
        "for i in range(num_total):\n",
        "    rann = np.random.random()\n",
        "    if rann < valid_frac:\n",
        "        valX.append(image_file_list[i])\n",
        "        valY.append(image_label_list[i])\n",
        "    elif rann < test_frac + valid_frac:\n",
        "        testX.append(image_file_list[i])\n",
        "        testY.append(image_label_list[i])\n",
        "    else:\n",
        "        trainX.append(image_file_list[i])\n",
        "        trainY.append(image_label_list[i])\n",
        "\n",
        "print(\"Training count =\",len(trainX),\"Validation count =\", len(valX), \"Test count =\",len(testX))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ThnxY_F1-c",
        "colab_type": "text"
      },
      "source": [
        "The MedNIST dataset now just needs to load the specified files. We use PIL to load the individual image file and convert it to a tensor afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J054LfzYPgLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from typing import Sequence, Dict\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MedNISTDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Simple dataset to load individual samples from the dataset\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, image_files: Sequence[str], labels: Sequence[int]):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      image_files: paths to the image files\n",
        "      labels: label for each file\n",
        "    \"\"\"\n",
        "    assert len(image_files) == len(labels), \"Every file needs a label\"\n",
        "    self.image_files = image_files\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Number of samples inside the dataset\n",
        "\n",
        "    Returns:\n",
        "      int: length\n",
        "    \"\"\"\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Select an individual sample from the dataset\n",
        "\n",
        "    Args:\n",
        "      index: index of sample to draw\n",
        "\n",
        "    Return:\n",
        "      Dict[str, torch.Tensor]: single sample\n",
        "        * `data`: image data\n",
        "        * `label`: label for sample\n",
        "    \"\"\"\n",
        "    data_np = np.array(Image.open(self.image_files[index]))\n",
        "    return {\"data\": torch.from_numpy(data_np)[None].float(),\n",
        "            \"label\": torch.tensor(self.labels[index]).long()}\n",
        "\n",
        "train_ds = MedNISTDataset(trainX, trainY)\n",
        "val_ds = MedNISTDataset(valX, valY)\n",
        "test_ds = MedNISTDataset(testX, testY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DO5p_kFHFil",
        "colab_type": "text"
      },
      "source": [
        "Let see some basic statistics of a single sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3pR83ziW2FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Single image min: {train_ds[0][\"data\"].min()}')\n",
        "print(f'Single image max: {train_ds[0][\"data\"].max()}')\n",
        "print(f'Single image mean: {train_ds[0][\"data\"].shape} (C, W, H)')\n",
        "print(f'Exaple label {train_ds[0][\"label\"]}')\n",
        "print(f'Example data: {train_ds[0][\"data\"]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJJWtrrKtE3",
        "colab_type": "text"
      },
      "source": [
        "The output could look something like this:\n",
        "```\n",
        "Single image min: 87.0\n",
        "Single image max: 255.0\n",
        "Single image mean: torch.Size([1, 64, 64]) (C, W, H)\n",
        "Exaple label 0\n",
        "Example data: tensor([[[101., 101., 101.,  ..., 101., 101., 101.],\n",
        "         [101., 101., 101.,  ..., 101., 101., 101.],\n",
        "         [101., 101., 101.,  ..., 101., 101., 101.],\n",
        "         ...,\n",
        "         [102., 101.,  99.,  ..., 111., 103.,  98.],\n",
        "         [102., 101., 100.,  ...,  99.,  98.,  98.],\n",
        "         [ 99., 100., 102.,  ..., 101., 103., 105.]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KclOdnMKlvB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8pFCTCdIIfa",
        "colab_type": "text"
      },
      "source": [
        "## Setting Up our Dataloading and Transformations\n",
        "In this section we will define our transformations and plug our dataset into the dataloader of `rising`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RheDTBMJdtE",
        "colab_type": "text"
      },
      "source": [
        "First we setup our transformation. In general these can be split into two parts: transformations which are applied as preprocessing and transformations which are applied as augmentations. All transformations are applied in a batched fashion to the dataset to fully utilize vectorization to speed up augmentation. In case your dataset needs additional preprocessing on a per sample basis you can also add those to the dataloder with `sample_transforms`. Check out or [3D Segmentation Tutorial]() for more infroamtion about that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQpBi2vwP0lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rising.transforms as rtr\n",
        "from rising.random import UniformParameter\n",
        "\n",
        "transforms_prep = []\n",
        "transforms_augment = []\n",
        "\n",
        "# preprocessing transforms\n",
        "# transforms_prep.append(rtr.NormZeroMeanUnitStd())\n",
        "# transforms_prep.append(rtr.NormMinMax()) # visualization looks nicer :) \n",
        "\n",
        "# augmentation transforms\n",
        "transforms_augment.append(rtr.GaussianNoise(0., 0.01))\n",
        "transforms_augment.append(rtr.GaussianSmoothing(\n",
        "    in_channels=1, kernel_size=3, std=0.5, padding=1))\n",
        "transforms_augment.append(rtr.Rot90((0, 1)))\n",
        "transforms_augment.append(rtr.Mirror(dims=(0, 1)))\n",
        "transforms_augment.append(rtr.BaseAffine(\n",
        "    scale=UniformParameter(0.8, 1.2),\n",
        "    rotation=UniformParameter(-30, 30), degree=True,\n",
        "    # translation in base affine is normalized to image size\n",
        "    # Translation transform offers to option to swith to pixels\n",
        "    translation=UniformParameter(-0.02, 0.02), \n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf_2-bXHLcYR",
        "colab_type": "text"
      },
      "source": [
        "In contrast to native PyTorch we add our transformations to the dataloder of rising. There are three main types of transformations which can be added:\n",
        "* `sample_transforms`: these transforms are applied per sample. In case the transformation assumes a batch of data `pseudo_batch_dim` can be activated to\n",
        "automatically add a batch dim to single samples.\n",
        "* `batch_transforms`: these transforms are executed per batch inside the multiprocessig context of the CPU (like `sample_transforms`).\n",
        "* `gpu_transforms`: these transforms are executed on the GPU. In case you have\n",
        "multiple GPUs make sure to set the correct `device`, otherwise rising could use\n",
        "the wrong GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQOFnEbWQB8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rising.loading import DataLoader\n",
        "\n",
        "tr_transform = rtr.Compose(transforms_prep + transforms_augment)\n",
        "dataloader_tr = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
        "                           gpu_transforms=tr_transform)\n",
        "\n",
        "val_transform = rtr.Compose(transforms_prep)\n",
        "dataloader_val = DataLoader(val_ds, batch_size=32,\n",
        "                            gpu_transforms=val_transform)\n",
        "\n",
        "test_transform = rtr.Compose(transforms_prep)\n",
        "dataloader_ts = DataLoader(test_ds, batch_size=32,\n",
        "                           gpu_transforms=test_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYQMemJMLWm0",
        "colab_type": "text"
      },
      "source": [
        "### Looking at some example outputs\n",
        "In this short section we will visualize some of the batches to look at the influence of the augmentations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-BvnKQVSu8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to visualize batches of images\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_batch(batch: torch.Tensor, norm: bool = True):\n",
        "  \"\"\"\n",
        "  Visualize a single batch of images\n",
        "\n",
        "  Args:\n",
        "    batch: batch of data\n",
        "    norm: normalized to range 0,1 for visualization purposes\n",
        "  \"\"\"\n",
        "  grid = torchvision.utils.make_grid(batch.cpu(), nrow=8)\n",
        "\n",
        "  grid -= grid.min()\n",
        "  m = grid.max()\n",
        "  if m > 1e-6:\n",
        "    grid = grid / m\n",
        "\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.imshow(grid[0], cmap='gray', vmin=0, vmax=1)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEluCh7uQkKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make dataset iterable\n",
        "_iter = iter(dataloader_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plgecEHcUM1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize batch of images\n",
        "batch = next(_iter)\n",
        "print({f'{key}_shape: {tuple(batch[key].shape)}' for key, item in batch.items()})\n",
        "print(f'Batch labels: \\n{batch[\"label\"]}')\n",
        "print(f'Batch mean {batch[\"data\"].mean()}')\n",
        "print(f'Batch min {batch[\"data\"].min()}')\n",
        "print(f'Batch max {batch[\"data\"].max()}')\n",
        "\n",
        "show_batch(batch[\"data\"], norm=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NMZnxKiRMpY",
        "colab_type": "text"
      },
      "source": [
        "The output of the visualization could look something like this:\n",
        "![Example Batch](https://drive.google.com/uc?id=1OEAcmtGJ5nQJLcu_BriL5zMIK_WxzHmZ)\n",
        "\n",
        "The exact images will vary because the batch was selected from the training dataloader which shuffles the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFB8j3gLP-k",
        "colab_type": "text"
      },
      "source": [
        "## Defining our Lightning Module\n",
        "We will use [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) as our trainer framework to save some time and to standardize our pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTmIVC_KS9QX",
        "colab_type": "text"
      },
      "source": [
        "In lightning the training models are derived from `pytorch_lightning.LightningModule` which enforces a specific structure of the code to increase reproducibility and stardization across the community. For simplicity we will simply load a torchvision model and overwrite the basic `*_step` functions of lightning. If you want more information how to build pipelines with pytorch lightning, please check out their [documentation](https://github.com/PyTorchLightning/pytorch-lightning). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnLgukDsUUy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from typing import Dict, Optional\n",
        "\n",
        "\n",
        "class SimpleClassifier(pl.LightningModule):\n",
        "  def __init__(self, hparams: Optional[dict] = None):\n",
        "    \"\"\"\n",
        "    Hyperparameters for our model\n",
        "\n",
        "    Args:\n",
        "      hparams: hyperparameters for model\n",
        "        `lr`: learning rate for optimizer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    if hparams is None:\n",
        "        hparams = {}\n",
        "    self.hparams = hparams\n",
        "    \n",
        "    # setup our model. For simplicity we will use a simple ResNet18\n",
        "    resnet = models.resnet18(pretrained=False)\n",
        "    # change first layer\n",
        "    resnet.conv1 = torch.nn.Conv2d(\n",
        "        1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # change last layer\n",
        "    fc_in = resnet.fc.in_features\n",
        "    resnet.fc = torch.nn.Linear(fc_in, num_class)\n",
        "    self.model = resnet\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Forward input batch of data through model\n",
        "\n",
        "    Args:\n",
        "      x: input batch of data [N, C, H, W]\n",
        "        N batch size (here 32); C number of channels (here 1);\n",
        "        H,W spatial dimensions of images (here 64x64)\n",
        "    \n",
        "    Returns:\n",
        "      torch.Tensor: classification logits [N, num_classes]\n",
        "    \"\"\"\n",
        "    return self.model(x)\n",
        "\n",
        "  def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Forward batch and compute loss for a single step (used for training)\n",
        "\n",
        "    Args:\n",
        "      batch: batch to process\n",
        "        `data`: input data\n",
        "        `label`: expected labels\n",
        "      batch_idx: index of batch\n",
        "    \"\"\"\n",
        "    x, y = batch[\"data\"], batch[\"label\"]\n",
        "    y_hat = self(x)\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    tensorboard_logs = {'train_loss': loss}\n",
        "    return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "  def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Forward batch and compute loss for a single step (used for validation)\n",
        "\n",
        "    Args:\n",
        "      batch: batch to process\n",
        "        `data`: input data\n",
        "        `label`: expected labels\n",
        "      batch_idx: index of batch\n",
        "    \"\"\"\n",
        "    x, y = batch[\"data\"], batch[\"label\"]\n",
        "    y_hat = self(x)\n",
        "    val_loss = F.cross_entropy(y_hat, y)\n",
        "    return {'val_loss': val_loss}\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    \"\"\"\n",
        "    Compute average validation loss during epoch\n",
        "    \"\"\"\n",
        "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    tensorboard_logs = {'val_loss': avg_loss}\n",
        "    return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "      \n",
        "  def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Forward batch and compute loss for a single step (used for validation)\n",
        "\n",
        "    Args:\n",
        "      batch: batch to process\n",
        "        `data`: input data\n",
        "        `label`: expected labels\n",
        "      batch_idx: index of batch\n",
        "    \"\"\"\n",
        "    x, y = batch[\"data\"], batch[\"label\"]\n",
        "    y_hat = self(x)\n",
        "    val_loss = F.cross_entropy(y_hat, y)\n",
        "    return {'test_loss': val_loss,\n",
        "            \"pred_label\": y_hat.max(dim=1)[1].detach().cpu(),\n",
        "            \"label\": y.detach().cpu()}\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "    \"\"\"\n",
        "    Compute average test loss and classification metrics\n",
        "    \"\"\"\n",
        "    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "    tensorboard_logs = {'test_loss': avg_loss}\n",
        "\n",
        "    all_pred_label = torch.cat([x['pred_label'] for x in outputs])\n",
        "    all_label = torch.cat([x['label'] for x in outputs])\n",
        "    print(classification_report(all_label.numpy(),\n",
        "                                all_pred_label.numpy(),\n",
        "                                target_names=class_names, digits=4))\n",
        "\n",
        "    return {'test_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    \"\"\"\n",
        "    Setup optimizer for training\n",
        "    \"\"\"\n",
        "    return torch.optim.Adam(self.parameters(), lr=self.hparams.get(\"lr\", 1e-5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbV8bfFjS8bE",
        "colab_type": "text"
      },
      "source": [
        "We can visualize our training progress and hyperparameters in tensorboard to easily compare multiple runs of our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYQi3zanWgde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWpsbBES38n",
        "colab_type": "text"
      },
      "source": [
        "Let's start our training :D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZh2Zf-0Vor2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "model = SimpleClassifier()\n",
        "\n",
        "# most basic trainer, uses good defaults\n",
        "trainer = Trainer(gpus=1, progress_bar_refresh_rate=10, max_epochs=4, weights_summary=None)\n",
        "trainer.fit(model, train_dataloader=dataloader_tr, val_dataloaders=dataloader_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMmIOVEJW-Wo",
        "colab_type": "text"
      },
      "source": [
        "After training our model we can test it on our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdX_PB6tXAjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.test(test_dataloaders=dataloader_ts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYGzjLyTNNM",
        "colab_type": "text"
      },
      "source": [
        "The results on the test data should look similar to this:\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "   AbdomenCT     0.9536    0.9990    0.9758      1008\n",
        "   BreastMRI     1.0000    1.0000    1.0000       830\n",
        "         CXR     0.9960    0.9872    0.9916      1013\n",
        "     ChestCT     1.0000    0.9490    0.9738       961\n",
        "        Hand     0.9877    0.9887    0.9882       975\n",
        "      HeadCT     0.9912    1.0000    0.9956      1019\n",
        "\n",
        "    accuracy                         0.9873      5806\n",
        "   macro avg     0.9881    0.9873    0.9875      5806\n",
        "weighted avg     0.9876    0.9873    0.9872      5806\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xya__f02k5xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}