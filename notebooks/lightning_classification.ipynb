{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Data\n",
      "Extracting Data\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tempfile\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "data_dir = os.path.join(temp_dir, 'Task_04_Hippocampus')\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    tar_path = os.path.join(temp_dir, 'data.tar')\n",
    "    if not os.path.exists(tar_path):\n",
    "        print('Downloading Data')\n",
    "        download_file_from_google_drive('1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C', \n",
    "                                        os.path.join(temp_dir, 'test.tar'))\n",
    "    print('Extracting Data')\n",
    "    tarfile.TarFile(os.path.join(temp_dir, 'test.tar')).extractall(temp_dir)\n",
    "    print('Success!')\n",
    "data_dir = os.path.join(temp_dir, 'Task04_Hippocampus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import json\n",
    "from rising import loading\n",
    "from rising.loading import Dataset\n",
    "import torch\n",
    "class NiiDataset(Dataset):\n",
    "    def __init__(self, train: bool, data_dir: str):\n",
    "        with open(os.path.join(data_dir, 'dataset.json')) as f:\n",
    "            content = json.load(f)['training']\n",
    "            num_train_samples = int(len(content) * 0.9)\n",
    "            if train:\n",
    "                data = content[:num_train_samples]\n",
    "            else:\n",
    "                data = content[num_train_samples:]\n",
    "            \n",
    "            self.data = data\n",
    "            self.data_dir = data_dir\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        sample = self.data[item]\n",
    "        img = sitk.GetArrayFromImage(\n",
    "            sitk.ReadImage(os.path.join(self.data_dir, sample['image'])))\n",
    "\n",
    "        # add channel dim if necesary\n",
    "        if img.ndim == 3:\n",
    "            img = img[None]\n",
    "\n",
    "        label = sitk.GetArrayFromImage(\n",
    "            sitk.ReadImage(os.path.join(self.data_dir, sample['label'])))\n",
    "        \n",
    "        # convert multiclass to binary task by combining all positives\n",
    "        label = label > 0\n",
    "        \n",
    "        # remove channel dim if necessary\n",
    "        if label.ndim == 3:\n",
    "            label = label[None]\n",
    "        return {'data': torch.from_numpy(img).float(), \n",
    "                'label': torch.from_numpy(label).float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class Unet(pl.LightningModule):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super().__init__()\n",
    "        # 4 downsample layers\n",
    "        out_filts = hparams.get('start_filts', 16)\n",
    "        depth = hparams.get('depth', 3)\n",
    "        in_filts = hparams.get('in_channels', 1)\n",
    "        num_classes = hparams.get('num_classes', 2)\n",
    "\n",
    "        for idx in range(depth):\n",
    "            down_block = torch.nn.Sequential(torch.nn.Conv3d(in_filts, out_filts, kernel_size=3, padding=1), torch.nn.ReLU(inplace=True),\n",
    "                                             torch.nn.Conv3d(out_filts, out_filts, kernel_size=3, padding=1), torch.nn.ReLU(inplace=True))\n",
    "            in_filts = out_filts\n",
    "            out_filts *= 2\n",
    "\n",
    "            setattr(self, 'down_block_%d' % idx, down_block)\n",
    "\n",
    "        out_filts = out_filts // 2\n",
    "        in_filts = in_filts // 2\n",
    "        out_filts, in_filts = in_filts, out_filts\n",
    "\n",
    "        for idx in range(depth-1):\n",
    "            up_block = torch.nn.Sequential(torch.nn.Conv3d(in_filts + out_filts, out_filts, kernel_size=3, padding=1), torch.nn.ReLU(inplace=True),\n",
    "                                            torch.nn.Conv3d(out_filts, out_filts, kernel_size=3, padding=1), torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            in_filts = out_filts\n",
    "            out_filts = out_filts // 2\n",
    "\n",
    "            setattr(self, 'up_block_%d' % idx, up_block)\n",
    "\n",
    "        self.final_conv = torch.nn.Conv3d(in_filts, num_classes, kernel_size=1)\n",
    "        self.max_pool = torch.nn.MaxPool3d(2, stride=2)\n",
    "        self.up_sample = torch.nn.Upsample(scale_factor=2)\n",
    "        self.hparams = hparams\n",
    "    \n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        depth = self.hparams.get('depth', 3)\n",
    "\n",
    "        intermediate_outputs = []\n",
    "\n",
    "        for idx in range(depth):\n",
    "            intermed = getattr(self, 'down_block_%d' % idx)(input_tensor)\n",
    "            if idx < depth - 1:\n",
    "                intermediate_outputs.append(intermed)\n",
    "                input_tensor = getattr(self, 'max_pool')(intermed)\n",
    "            else:\n",
    "                input_tensor = intermed\n",
    "\n",
    "        for idx in range(depth-1):\n",
    "            input_tensor = getattr(self, 'up_sample')(input_tensor)\n",
    "            from_down = intermediate_outputs.pop(-1)\n",
    "            intermed = torch.cat([input_tensor, from_down], dim=1)\n",
    "            input_tensor = getattr(self, 'up_block_%d' % idx)(intermed)\n",
    "\n",
    "        return getattr(self, 'final_conv')(input_tensor)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "net = Unet({'num_classes': 2, 'in_channels': 1, 'depth': 3})\n",
    "print(net(torch.rand(1, 1, 128, 128, 128)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rising\n",
    "\n",
    "# Taken from https://github.com/justusschock/dl-utils/blob/master/dlutils/losses/soft_dice.py\n",
    "class SoftDiceLoss(torch.nn.Module):\n",
    "    def __init__(self, square_nom=False, square_denom=False, weight=None,\n",
    "                 smooth=1., reduction=\"elementwise_mean\", non_lin=None):\n",
    "        \"\"\"\n",
    "        SoftDice Loss\n",
    "        Parameters\n",
    "        ----------\n",
    "        square_nom : bool\n",
    "            square nominator\n",
    "        square_denom : bool\n",
    "            square denominator\n",
    "        weight : iterable\n",
    "            additional weighting of individual classes\n",
    "        smooth : float\n",
    "            smoothing for nominator and denominator\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.square_nom = square_nom\n",
    "        self.square_denom = square_denom\n",
    "\n",
    "        self.smooth = smooth\n",
    "\n",
    "        if weight is not None:\n",
    "            self.register_buffer(\"weight\", torch.tensor(weight))\n",
    "        else:\n",
    "            self.weight = None\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.non_lin = non_lin\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Compute SoftDice Loss\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.Tensor\n",
    "            prediction\n",
    "        targets : torch.Tensor\n",
    "            ground truth tensor\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            loss\n",
    "        \"\"\"\n",
    "        # number of classes for onehot\n",
    "        n_classes = predictions.shape[1]\n",
    "        with torch.no_grad():\n",
    "            targets_onehot = rising.transforms.functional.channel.one_hot_batch(\n",
    "                targets.unsqueeze(1), num_classes=n_classes)\n",
    "        # sum over spatial dimensions\n",
    "        dims = tuple(range(2, predictions.dim()))\n",
    "\n",
    "        # apply nonlinearity\n",
    "        if self.non_lin is not None:\n",
    "            predictions = self.non_lin(predictions)\n",
    "\n",
    "        # compute nominator\n",
    "        if self.square_nom:\n",
    "            nom = torch.sum((predictions * targets_onehot.float()) ** 2, dim=dims)\n",
    "        else:\n",
    "            nom = torch.sum(predictions * targets_onehot.float(), dim=dims)\n",
    "        nom = 2 * nom + self.smooth\n",
    "\n",
    "        # compute denominator\n",
    "        if self.square_denom:\n",
    "            i_sum = torch.sum(predictions ** 2, dim=dims)\n",
    "            t_sum = torch.sum(targets_onehot ** 2, dim=dims)\n",
    "        else:\n",
    "            i_sum = torch.sum(predictions, dim=dims)\n",
    "            t_sum = torch.sum(targets_onehot, dim=dims)\n",
    "\n",
    "        denom = i_sum + t_sum.float() + self.smooth\n",
    "\n",
    "        # compute loss\n",
    "        frac = nom / denom\n",
    "\n",
    "        # apply weight for individual classesproperly\n",
    "        if self.weight is not None:\n",
    "            frac = self.weight * frac\n",
    "\n",
    "        # average over classes\n",
    "        frac = - torch.mean(frac, dim=1)\n",
    "\n",
    "        return frac\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/justusschock/dl-utils/blob/master/dlutils/metrics/dice.py\n",
    "def binary_dice_coefficient(pred: torch.Tensor, gt: torch.Tensor,\n",
    "                            thresh: float = 0.5, smooth: float = 1e-7):\n",
    "    \"\"\"\n",
    "    A binary dice coefficient\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : torch.Tensor\n",
    "        predicted segmentation (of shape NxCx(Dx)HxW)\n",
    "    gt : torch.Tensor\n",
    "        target segmentation (of shape NxCx(Dx)HxW)\n",
    "    thresh : float\n",
    "        segmentation threshold\n",
    "    smooth : float\n",
    "        smoothing value to avoid division by zero\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        dice score\n",
    "    \"\"\"\n",
    "    pred_bool = pred > thresh\n",
    "\n",
    "    intersec = (pred_bool * gt).float()\n",
    "    return 2 * intersec.sum() / (pred_bool.float().sum()\n",
    "                                 + gt.float().sum() + smooth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rising.transforms import Compose, ResizeNative\n",
    "\n",
    "def common_per_sample_trafos():\n",
    "        return Compose(ResizeNative(size=(32, 64, 32), keys=('data',), mode='trilinear'),\n",
    "                        ResizeNative(size=(32, 64, 32), keys=('label',), mode='nearest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from rising.transforms.affine import BaseAffine\n",
    "import random\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "class RandomAffine(BaseAffine):\n",
    "    def __init__(self, scale_range: Optional[tuple] = None, \n",
    "                 rotation_range: Optional[tuple] = None, \n",
    "                 translation_range: Optional[tuple] = None,\n",
    "                 degree: bool = True,\n",
    "                 image_transform: bool = True,\n",
    "                 keys: Sequence = ('data',),\n",
    "                 grad: bool = False,\n",
    "                 output_size: Optional[tuple] = None,\n",
    "                 adjust_size: bool = False,\n",
    "                 interpolation_mode: str = 'nearest',\n",
    "                 padding_mode: str = 'zeros',\n",
    "                 align_corners: bool = False,\n",
    "                 reverse_order: bool = False,\n",
    "                 **kwargs,):\n",
    "        super().__init__(scale=None, rotation=None, translation=None, \n",
    "                         degree=degree,\n",
    "                         image_transform=image_transform, \n",
    "                         keys=keys, \n",
    "                         grad=grad, \n",
    "                         output_size=output_size, \n",
    "                         adjust_size=adjust_size, \n",
    "                         interpolation_mode=interpolation_mode, \n",
    "                         padding_mode=padding_mode, \n",
    "                         align_corners=align_corners, \n",
    "                         reverse_order=reverse_order, \n",
    "                         **kwargs)\n",
    "        \n",
    "        self.scale_range = scale_range\n",
    "        self.rotation_range = rotation_range\n",
    "        self.translation_range = translation_range\n",
    "        \n",
    "    def assemble_matrix(self, **data):\n",
    "        ndim = data[self.keys[0]].ndim - 2\n",
    "        \n",
    "        if self.scale_range is not None:\n",
    "            self.scale = [random.uniform(*self.scale_range) for _ in range(ndim)]\n",
    "            \n",
    "        if self.translation_range is not None:\n",
    "            self.translation = [random.uniform(*self.translation_range) for _ in range(ndim)]\n",
    "            \n",
    "        if self.rotation_range is not None:\n",
    "            if ndim == 3:\n",
    "                self.rotation = [random.uniform(*self.rotation_range) for _ in range(ndim)]\n",
    "            elif ndim == 1:\n",
    "                self.rotation = random.uniform(*self.rotation_range)\n",
    "            \n",
    "        return super().assemble_matrix(**data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rising.transforms import NormZeroMeanUnitStd\n",
    "from rising.loading import DataLoader\n",
    "import torch\n",
    "class TrainableUNet(Unet):\n",
    "    def __init__(self, hparams: Optional[dict] = None):\n",
    "        if hparams is None:\n",
    "            hparams = {}\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.dice_loss = SoftDiceLoss(weight=[0., 1.])\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = NiiDataset(train=True, data_dir=data_dir)\n",
    "        \n",
    "        batch_transforms = Compose([\n",
    "            RandomAffine(scale_range=(self.hparams.get('min_scale', 0.9), self.hparams.get('max_scale', 1.1)),\n",
    "                         rotation_range=(self.hparams.get('min_rotation', -10), self.hparams.get('max_rotation', 10)),\n",
    "                        keys=('data',)),\n",
    "            NormZeroMeanUnitStd(keys=('data',))\n",
    "        ])\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=self.hparams.get('batch_size', 1),\n",
    "                                batch_transforms=batch_transforms,\n",
    "                                shuffle=True, \n",
    "                                sample_transforms=common_per_sample_trafos(),\n",
    "                                pseudo_batch_dim=True,\n",
    "                                num_workers=self.hparams.get('num_workers', 4))\n",
    "        return dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = NiiDataset(train=False, data_dir=data_dir)\n",
    "        \n",
    "        batch_transforms = NormZeroMeanUnitStd(keys=('data',))\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=self.hparams.get('batch_size', 1),\n",
    "                                batch_transforms=batch_transforms,\n",
    "                                shuffle=False, \n",
    "                                sample_transforms=common_per_sample_trafos(),\n",
    "                                pseudo_batch_dim=True,\n",
    "                                num_workers=self.hparams.get('num_workers', 4))\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.get('learning_rate', 1e-3))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['data'], batch['label']\n",
    "        \n",
    "        # remove channel dim from gt (was necessary for augmentation)\n",
    "        y = y[:, 0].long()\n",
    "        \n",
    "        pred = self(x)\n",
    "        softmaxed_pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        \n",
    "        ce_loss = self.ce_loss(pred, y)\n",
    "        dice_loss = self.dice_loss(softmaxed_pred, y)\n",
    "        total_loss = (ce_loss + dice_loss) / 2\n",
    "        \n",
    "        dice_coeff = binary_dice_coefficient(torch.argmax(softmaxed_pred, dim=1), y)\n",
    "        \n",
    "        self.logger.experiment.add_scalar('Train/DiceCoeff', dice_coeff)\n",
    "        self.logger.experiment.add_scalar('Train/CE', ce_loss)\n",
    "        self.logger.experiment.add_scalar('Train/SoftDiceLoss', dice_loss)\n",
    "        self.logger.experiment.add_scalar('Train/TotalLoss', total_loss)\n",
    "        \n",
    "        return {'loss': total_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['data'], batch['label']\n",
    "        \n",
    "        # remove channel dim from gt (was necessary for augmentation)\n",
    "        y = y[:, 0].long()\n",
    "        \n",
    "        pred = self(x)\n",
    "        softmaxed_pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        \n",
    "        ce_loss = self.ce_loss(pred, y)\n",
    "        dice_loss = self.dice_loss(softmaxed_pred, y)\n",
    "        total_loss = (ce_loss + dice_loss) / 2\n",
    "        \n",
    "        dice_coeff = binary_dice_coefficient(torch.argmax(softmaxed_pred, dim=1), y)\n",
    "        \n",
    "        self.logger.experiment.add_scalar('Val/DiceCoeff', dice_coeff)\n",
    "        self.logger.experiment.add_scalar('Val/CE', ce_loss)\n",
    "        self.logger.experiment.add_scalar('Val/SoftDiceLoss', dice_loss)\n",
    "        self.logger.experiment.add_scalar('Val/TotalLoss', total_loss)\n",
    "        \n",
    "        return {'val_loss': total_loss, 'dice': dice_coeff}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mean_outputs = {}\n",
    "        for k in outputs[0].keys():\n",
    "            mean_outputs[k] = torch.stack([x[k] for x in outputs]).mean()\n",
    "        return mean_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cf08456e9dfb9aed\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cf08456e9dfb9aed\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {temp_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: False, used: False\n",
      "INFO:lightning:\n",
      "   | Name           | Type             | Params\n",
      "------------------------------------------------\n",
      "0  | down_block_0   | Sequential       | 7 K   \n",
      "1  | down_block_0.0 | Conv3d           | 448   \n",
      "2  | down_block_0.1 | ReLU             | 0     \n",
      "3  | down_block_0.2 | Conv3d           | 6 K   \n",
      "4  | down_block_0.3 | ReLU             | 0     \n",
      "5  | down_block_1   | Sequential       | 41 K  \n",
      "6  | down_block_1.0 | Conv3d           | 13 K  \n",
      "7  | down_block_1.1 | ReLU             | 0     \n",
      "8  | down_block_1.2 | Conv3d           | 27 K  \n",
      "9  | down_block_1.3 | ReLU             | 0     \n",
      "10 | down_block_2   | Sequential       | 166 K \n",
      "11 | down_block_2.0 | Conv3d           | 55 K  \n",
      "12 | down_block_2.1 | ReLU             | 0     \n",
      "13 | down_block_2.2 | Conv3d           | 110 K \n",
      "14 | down_block_2.3 | ReLU             | 0     \n",
      "15 | up_block_0     | Sequential       | 110 K \n",
      "16 | up_block_0.0   | Conv3d           | 82 K  \n",
      "17 | up_block_0.1   | ReLU             | 0     \n",
      "18 | up_block_0.2   | Conv3d           | 27 K  \n",
      "19 | up_block_0.3   | ReLU             | 0     \n",
      "20 | up_block_1     | Sequential       | 27 K  \n",
      "21 | up_block_1.0   | Conv3d           | 20 K  \n",
      "22 | up_block_1.1   | ReLU             | 0     \n",
      "23 | up_block_1.2   | Conv3d           | 6 K   \n",
      "24 | up_block_1.3   | ReLU             | 0     \n",
      "25 | final_conv     | Conv3d           | 34    \n",
      "26 | max_pool       | MaxPool3d        | 0     \n",
      "27 | up_sample      | Upsample         | 0     \n",
      "28 | dice_loss      | SoftDiceLoss     | 0     \n",
      "29 | ce_loss        | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justusschock/Workspace/conda/envs/rising/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/justusschock/Workspace/conda/envs/rising/lib/python3.6/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/260 [00:00<?, ?it/s]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justusschock/Workspace/conda/envs/rising/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 8/260 [00:12<06:34,  1.57s/it, loss=0.242, v_num=12]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='dice', min_delta=0.001, patience=3, verbose=False, mode='max')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpus = 0\n",
    "else:\n",
    "    gpus = None\n",
    "\n",
    "model = TrainableUNet({'num_workers': 0})\n",
    "\n",
    "trainer = Trainer(gpus=gpus, default_save_path=temp_dir, early_stop_callback=early_stop_callback, max_nb_epochs=5)\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('rising': conda)",
   "language": "python",
   "name": "python361064bitrisingconda39a2201ad26e4c96be348e851446357a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
